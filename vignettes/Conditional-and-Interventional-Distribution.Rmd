---
title: "Conditional-and-Interventional-Distribution"
output: rmarkdown::html_vignette
bibliography: bibliography.bib
vignette: >
  %\VignetteIndexEntry{Conditional-and-Interventional-Distribution}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup}
library(bnSEM)
```

Bayesian Networks allow investigating the conditional and interventional 
distribution of network nodes. 

The central difference between conditional and interventional distribution
can be explained with a variation of the classical sprinkler-rain model [see e.g.,
@murphy2012machine, p. 944]. Assume that R denotes rain, W is the weather report,
and S is the activation of water sprinklers. In the model, both the weather report
and the activation of sprinklers are affected by rain:

```{r, echo=FALSE}
par(oma = rep(0, 4), mar = rep(0, 4), mai = rep(0, 4),
  plt = c(0.06, 0.94, 0.12, 0.88))
bnlearn::model2network("[R][W|R][S|R]") |>
  plot()
```



- The conditional distribution $P(W = \text{rain} |S  = \text{activated})$ will tell us the probability 
of the weather report reporting rain on a day where we observe that the sprinklers are activated. 
Because sprinklers will typically not be activated on days where it rains, we will know that
the weather forecast will probably also report rain.
- The interventional distribution $P(W | do(s))$ will tell us the probability 
of the weather report reporting rain when we activate (i.e., intervene on) the
sprinklers. Of course, activating or deactivating the sprinkler will not have any
effect on the weather report because the connections between both is caused by a 
third variable.

This can be demonstrated with bnlearn [@scutari2009learning].

```{r}
# the following code is adapted from https://www.bnlearn.com/examples/custom-fitted/
library(bnlearn)
C <- matrix(c(0.2, 0.8), 
            ncol = 2, 
            dimnames = list(NULL, c("Cloudy", "Not Cloudy")))

R <- matrix(data = c(.7, .1,
                     .3, .9), 
            nrow = 2,
            ncol = 2,
            byrow = TRUE,
            dimnames = list(c("Rain", "No Rain"),
                            c("Cloudy", "Not Cloudy")))

S <- matrix(data = c(.2, .8,
                     .8, .2), 
            nrow = 2,
            ncol = 2,
            byrow = TRUE,
            dimnames = list(c("Sprinkler", "No Sprinkler"),
                            c("Cloudy", "Not Cloudy")))

net = model2network("[C][R|C][S|C]")
fit = custom.fit(net, dist = list(C = C, 
                                  R = R, 
                                  S = S))
print(fit)
```

Let's first investigate the conditional distribution of rain given that the 
sprinkler is on:

```{r}
cpquery(fitted = fit, 
        event = (R == "Rain"), 
        evidence = (S == "Sprinkler"))
```

The probability of rain given the sprinklers are on is .132. 

```{r}
cpquery(fitted = fit, 
        event = (R == "No Rain"), 
        evidence = (S == "Sprinkler"))
```

The probability of no rain given the sprinklers are on is 0.863.

For the interventional distribution we have to look at the so-called mutilated
network (see `?bnlearn::mutilated`). The basic idea here is that intervening
on the sprinkler will cut any connection of the sprinkler and its parent nodes 
(here cloudiness).

```{r}
mut <- mutilated(x = fit, evidence = 
                   list("S" = "Sprinkler"))
print(mut)
```
Note how the node S is no longer affected by cloudiness. The interventional
distribution of rain is now given by:
```{r}
cpquery(fitted = mut, 
        event = (R == "No Rain"), 
        evidence = (S == "Sprinkler"))

cpquery(fitted = mut, 
        event = (R == "No Rain"), 
        evidence = (S == "Sprinkler"))
```

Note how the probability of rain is now unaffected by the status of the 
sprinkler (small differences between the probabilities are due to the sampling 
procedure used with cpquery).

## Structural Equation Models

The difference between conditional and interventional distribution in SEM is discussed in
detail by @gische2022beyond.

To apply the ideas outlines above to an SEM, we will use a the Political Democracy
model fitted with OpenMx [@neale2016openmx]. The model is given by:

```{r, message = FALSE}
library(mxsem)
model <- '
  # latent variable definitions
     ind60 =~ x1 + x2 + x3
     dem60 =~ y1 + a*y2 + b*y3 + c*y4
     dem65 =~ y5 + a*y6 + b*y7 + c*y8

  # regressions
    dem60 ~ ind60
    dem65 ~ ind60 + dem60

  # residual correlations
    y1 ~~ y5
    y2 ~~ y4 + y6
    y3 ~~ y7
    y4 ~~ y8
    y6 ~~ y8
'

mx_model <- mxsem(model,
                  data = OpenMx::Bollen) |>
  OpenMx::mxTryHard()
```

First, we translate the model to a Bayesian network:
```{r, message = FALSE}
library(bnSEM)
network <- bnSEM::bnSEM(mx_model = mx_model)
```

Next, we want to investigate the conditional distribution of dem65 given 
y6 = 1. Here, we will use the cpdist function to generate samples.

```{r}
cond_dist <- bnlearn::cpdist(fitted = network$bayes_net, 
                             nodes = "dem65", 
                             evidence = list("y6" = 1), 
                             method = "lw")
# Using likelihood weighting, we have to take the weights into account when
# computing the expected value and the variance
(m <- sum(cond_dist$dem65 * attr(cond_dist, "weights")) / sum(attr(cond_dist, "weights")))
sum(attr(cond_dist, "weights") * (cond_dist$dem65 - m)^2)/ 
  sum(attr(cond_dist, "weights"))
```

To get the interventional distribution, we can again use the mutilated network:
```{r}
mut <- mutilated(x = network$bayes_net, 
                 evidence = list("y6" = 1))
inter_dist <- cpdist(fitted = mut,
                     nodes = "dem65",
                     evidence = TRUE,
                     method = "lw")
(m <- sum(inter_dist$dem65 * attr(inter_dist, "weights")) / sum(attr(inter_dist, "weights")))
sum(attr(inter_dist, "weights") * (inter_dist$dem65 - m)^2)/ 
  sum(attr(inter_dist, "weights"))
```
Note that this is identical to the expected value of dem65 (0; with some numerical 
imprecision due to sampling).

Let's look at another example. To this end, we will first check the mean and
variance of x1 in our network.
```{r}
dist <- bnlearn::cpdist(fitted = network$bayes_net, 
                        nodes = "x1", 
                        evidence = TRUE, 
                        method = "lw")
# Using likelihood weighting, we have to take the weights into account when
# computing the expected value and the variance
(m <- sum(dist$x1 * attr(dist, "weights")) / sum(attr(dist, "weights")))
sum(attr(dist, "weights") * (dist$x1 - m)^2)/ 
  sum(attr(dist, "weights"))
```

Now, let's check what the conditional mean and variance of x1 are given that
dem60 = 3
```{r}
cond_dist <- bnlearn::cpdist(fitted = network$bayes_net, 
                             nodes = "x1", 
                             evidence = list("dem60" = 3), 
                             method = "lw")
# Using likelihood weighting, we have to take the weights into account when
# computing the expected value and the variance
(m <- sum(cond_dist$x1 * attr(cond_dist, "weights")) / sum(attr(cond_dist, "weights")))
sum(attr(cond_dist, "weights") * (cond_dist$x1 - m)^2)/ 
  sum(attr(cond_dist, "weights"))
```
Note how knowledge about dem60 provides us with information about the indicator
x1 of ind60. 

When intervening on dem60, in contrast, we don't change our expectation for x1:
```{r}
mut <- mutilated(x = network$bayes_net, 
                 evidence = list("dem60" = 10))
inter_dist <- cpdist(fitted = mut,
                     nodes = "x1",
                     evidence = TRUE,
                     method = "lw")
(m <- sum(inter_dist$x1 * attr(inter_dist, "weights")) / sum(attr(inter_dist, "weights")))
sum(attr(inter_dist, "weights") * (inter_dist$x1 - m)^2)/ 
  sum(attr(inter_dist, "weights"))
```

# Bibliography
