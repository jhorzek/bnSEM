% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/cov_to_phantom.R
\name{cov_to_phantom_cholesky}
\alias{cov_to_phantom_cholesky}
\title{cov_to_phantom_cholesky}
\usage{
cov_to_phantom_cholesky(parameter_table, mx_model)
}
\arguments{
\item{parameter_table}{parameter table of an OpenMx model (see ?OpenMx::omxLocateParameters)}

\item{mx_model}{fitted OpenMx model}
}
\value{
fitted mx_model with phantom variables
}
\description{
In Bayesian networks, all dependencies between variables must be expressed with
directed "effects". There is no covariance in the same sense as in OpenMx.
Instead, we must replace covariances with effects of a latent phantom variable.
}
\details{
cov_to_phantom_cholesky uses a Cholesky decomposition to represent the covariance
matrix of the entire model. This approach has been suggested by Manuel C. Voelkle.
See, for example, p. 462 in
Klein, A., & Moosbrugger, H. (2000). Maximum likelihood estimation of latent
interaction effects with the LMS method. Psychometrika, 65(4), 457-474.

The main advantage of using a Cholesky decomposition is that no refitting of the
model is required. As a result, many convergence issues are avoided.

The main disadvantage of the Cholesky decomposition approach is that we are not
just "outsourcing" the covariances to new phantom variables, but also the variances.
As a result, all observed and latent variables in the original model are now
fully determined by new latent variables. This results in issues with bnlearn,
where conditional distributions cannot be computed.
}
\keyword{internal}
